[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Gianluca Rigoletti\nI am a senior fellow physicist working at CERN. My current work if focused around the maintenance and operation of gas systems for particle detectors. A significant part of this work consists of engineering and sizing gas systems, but also develop the industrial control software required to operate the plants. I also work a lot with python to wrangle, analyze and visualize data from such systems. Another part of my activities is revolves around the study of different strategies to reduce Greenhouse gas emissions from particle detectors. This could go from optimizing current gas systems to test alternative gases for particle detectors employing impactful greenhouse gases.\nI have few hobbies I am taking care of in my free time:\n\nMaking pizza: since 2019 I started to be systematic in the way I cook pizza as I wanted to try my best to achieve a good napolitean pizza at my place.\nIoT devices for plants based experiments. I test time to times micro controllers and different sensors to grow some herbs in my indoor gardens.\nEating and cooking: I particularly enjoy eating in places putting their best effort in sustainability of their product. I am a big lovers of tomatoes growing in the middle of Italy\nFew other things I can’t think of right now\n\nI am blogging in English but I speak Italian and a bit of French as well.\n\n\nEducation\nPhD in particle physics | 2019 - 2022\nUniversité Claude Bernard Lyon I\nMaster degree in physics of the matter | 2017-2019\nUniversità degli Studi di Milano-Bicocca\nBachelor degree in physics of the matter | 2017-2019\nUniversità degli Studi di Milano-Bicocca\n\n\nExperience\nCERN Senior fellow | 2022 CERN, Switzerland\nSoftware developer | 2016-2017 Yperesia, Italy"
  },
  {
    "objectID": "posts/a-post-with-jupyter-notebook/index.html",
    "href": "posts/a-post-with-jupyter-notebook/index.html",
    "title": "A post made from a jupyter notebook",
    "section": "",
    "text": "Few days ago I found Quarto. I understood it is an authoring framework based on pandoc and that has full support of jupyter notebooks as input files. Since lot of work I carry on at the moment makes a heavy use of jupyter notebook I decided to try Quarto and use it for two main purposes: the first is as a replacement of jupyter nbconvert that I have intensively used to convert notebooks to html pages.\nI saw that Quarto provides much more flexibility on the layout of the text, images and code, which was exactly what I was looking for. The second way I intend to use Quarto is to publish the present content. At the time of writing, this post is written on an index.ipynb file in my local computer using Visual Studio code. I find extremely pleasant to write mixed content and have a tool that transforms it in a post page of a blog website.\nI will showcase here all the features that I have personally found relevant for the posts I am going to write as well, hoping that this post serves as a quick reference (together with the Quarto user guide)."
  },
  {
    "objectID": "posts/a-post-with-jupyter-notebook/index.html#code-cells",
    "href": "posts/a-post-with-jupyter-notebook/index.html#code-cells",
    "title": "A post made from a jupyter notebook",
    "section": "Code cells",
    "text": "Code cells\nCode cells are pretty straightforward: I just have to write code in a cell and it will be executed and the output eventually frozen by Quarto. I found the freeze option particularly useful and I set it to auto at the time of writing. In this way, the notebooks will be re-rendered only if the source code changes.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\n\nAt the very base you can just enter some code and save it. If you are using quarto preview the jupyter notebook is executed when it saved (and not when a code cell is executed, but this depends on the saving options of jupyter).\nFor instance, we can write some code and let it display some output like this:\n\nclass TestClass:\n    def __init__(self, params) -&gt; None:\n        self.attributes = params\n    \n    def increment(self, param: int) -&gt; int:\n        return param + 1\n\nobj = TestClass('test')\nobj.increment(10)\n\n11\n\n\nSometimes a cell may contain lot of code that makes it difficult for the reader to keep track of the text. In this case folding the code is very helpful and only requires a special comment to be added at the beginning of the code cell:\n#| code-fold: true\n\nclass TestClass:\n    def __init__(self, params) -&gt; None:\n        self.attributes = params\n    \n    def increment(self, param: int) -&gt; int:\n        return param + 1\n\nobj = TestClass('test')\nobj.increment(10)\nHere below the result of the previous snippet of code:\n\n\nCode\nclass TestClass:\n    def __init__(self, params) -&gt; None:\n        self.attributes = params\n    \n    def increment(self, param: int) -&gt; int:\n        return param + 1\n\nobj = TestClass('test')\nobj.increment(10)\n\n\n11\n\n\nNote that the output of the cell is not folded. This is particularly useful if you have lot of code printing out a plot for example: the code will be folded but the plot will be visible anyway.\nSome other time I wish to completely hide the code while keeping the output visible. In this case the option echo: false does the job:\n#| echo: false\nplt.plot([1,2,3], [4,5,6], 'ro');\nHere below the result of the previous snippet of code:\n\n\n\n\n\nCode cells can be also assigned a file name. This is particularly helpful when there is a post that needs to tackle code among several files. As an example you can have something like:\n#| filename: test.py\n\nclass A:\n    pass\nAnd this would be rendered to:\n\n\n\ntest.py\n\nclass A:\n    pass"
  },
  {
    "objectID": "posts/a-post-with-jupyter-notebook/index.html#controlling-figures-layout",
    "href": "posts/a-post-with-jupyter-notebook/index.html#controlling-figures-layout",
    "title": "A post made from a jupyter notebook",
    "section": "Controlling figures layout",
    "text": "Controlling figures layout\nI find particularly useful the way Quarto helps treating outputs for code cells, especially laying out figures resulting from a code output. Often times I have big plots that I would like to take all the available horizontal space in the page. In this case I can use #| column: page together with on the code cell\n#| column: page\n#| fig-align: center\nfig, ax = plt.subplots(figsize=(8, 5), dpi=150)\nax.plot(range(10), [1,2,3,2,1,2,2,31,1,2])\n\n\n\n\n\nThere is also support for custom layouts when multiple plots are produced. I typically use matplotlib APIs to lay out multiple plots but this features could be useful for images in general and sometimes with libraries where the layout support for subplots is not straightforward. As an example see this snippet:\n#| layout-ncol: 2\n\nfig, ax = plt.subplots()\nax.plot([1,2,3,4], [4,3,2,3], 'ro-')\n\nfig2, ax2 = plt.subplots()\nax2.plot([1,2,3,4], [4,3,1,32], 'b*--')"
  },
  {
    "objectID": "posts/a-post-with-jupyter-notebook/index.html#conclusions",
    "href": "posts/a-post-with-jupyter-notebook/index.html#conclusions",
    "title": "A post made from a jupyter notebook",
    "section": "Conclusions",
    "text": "Conclusions\nThere are several more features that I think I will be using throughout my journey on this blog. There are also some features related to publications that I want to explore as I would like to understand if I can have some publications ready LaTeX documents from jupyter notebooks\nI found Quarto really easy to use and the documentation is fairly straightforward to follow. I hope to post more interesting findings and customization as I will use it :)"
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html",
    "href": "posts/tabulator-sql-fastapi/index.html",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "",
    "text": "At work we have an SQL database containing a list of alarms that are generated by different control systems. We usually receive Email and SMS notifications for that, but it’s not easy to get an overview of all the alarms. We decided then to create a simple web application that wraps the data on the database and allows people from the team to easily access it.\nWhen thinking about the web applications I wanted to have the following requirements:\n\nThe alarms data should be only read, no modifications are required\nThe alarms data should be loaded with server side pagination, since the number of rows in the DB are ~O(\\(10^4\\)) and I don’t want to load them on the web page each time\nThe data should be displayed on a table that allows for custom filtering. The only way to do filtering with server side pagination is for the table to send the filter queries on the backend and do some sort of server side filtering\nOne of the columns should contains a link that open a page with all the details of the clicked alarm, like in a RESTful resource\n\n\n\nI initially evaluated the possibility of using some python libraries like streamlit, panel, h2o-wave, plotly’s dash, each one with a set of pros and cons. I decided to not use them because I felt that although using a data table is pretty easy (panel and dash support the use of tabulator for example), the server side pagination and filtering was a bit cumbersome. In addition, integrating the application with a python ASGI server started to be a bit too complicated to the point that having a single framework seemed the cleanest solution to me.\nI have also considered few no-code tools that wrap any data around a web application. I have considered Apache’s superset which is a very powerful tool but it was probably an overkill. I have also though about using directus which works pretty ok but it adds metadata tables on your db and the filtering on the frontend is a bit cumbersome.\nI ultimately decided to invest some time and go back to the basics of python web frameworks and vanilla javascript libraries. In the past I used FastAPI for a RESTful web application and I found it quite enjoyable, especially for its clear and extensive documentation. Since the project partially requires to serve data from an REST interface I decided to go for it and use it again.\nOn the frontend I started googling for javascript data tables. I also saw that streamlit had a plugin for AgGrid which seemed like an extremely powerful library but that requires a quite pricy license, so I decided to discard it. Some other options included DataTables and GridJs but I ultimately ended up trying tabulator because I liked the documentation and the set of examples it provided."
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#alternative-tools",
    "href": "posts/tabulator-sql-fastapi/index.html#alternative-tools",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "",
    "text": "I initially evaluated the possibility of using some python libraries like streamlit, panel, h2o-wave, plotly’s dash, each one with a set of pros and cons. I decided to not use them because I felt that although using a data table is pretty easy (panel and dash support the use of tabulator for example), the server side pagination and filtering was a bit cumbersome. In addition, integrating the application with a python ASGI server started to be a bit too complicated to the point that having a single framework seemed the cleanest solution to me.\nI have also considered few no-code tools that wrap any data around a web application. I have considered Apache’s superset which is a very powerful tool but it was probably an overkill. I have also though about using directus which works pretty ok but it adds metadata tables on your db and the filtering on the frontend is a bit cumbersome.\nI ultimately decided to invest some time and go back to the basics of python web frameworks and vanilla javascript libraries. In the past I used FastAPI for a RESTful web application and I found it quite enjoyable, especially for its clear and extensive documentation. Since the project partially requires to serve data from an REST interface I decided to go for it and use it again.\nOn the frontend I started googling for javascript data tables. I also saw that streamlit had a plugin for AgGrid which seemed like an extremely powerful library but that requires a quite pricy license, so I decided to discard it. Some other options included DataTables and GridJs but I ultimately ended up trying tabulator because I liked the documentation and the set of examples it provided."
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#create-a-database-and-populate-it-with-some-data",
    "href": "posts/tabulator-sql-fastapi/index.html#create-a-database-and-populate-it-with-some-data",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "Create a database and populate it with some data",
    "text": "Create a database and populate it with some data\nFor simplicity I am going to use sqlite, but any popular SQL database should work fine. Note that I am not using any ORM to keep it as simple as possible. In this example, the database will consists of a single table alarms with the following columns:\n\nid\nsystem\ntimestamp\ncategory\ntext\n\nWe can create a file in the app folder to insert some simple synthetic data:\n\n\napp/db.py\n\nimport sqlite3\nimport random\nfrom datetime import datetime, timedelta\nimport sys\n\n\ndef create_db(db: str, table: str):\n    conn = sqlite3.connect(db)\n    cur = conn.cursor()\n    cur.execute(f\"DROP TABLE IF EXISTS {table};\")\n    cur.execute(f\"\"\"CREATE TABLE {table}(\n        id integer primary key autoincrement, \n        system, timestamp DATETIME, category, text);\"\"\")\n    conn.commit()\n\nThe function above will create a table alarms on the provided sqlite db filepath. Next, we can create a function to generate data\n\n\napp/db.py\n\n...\n\ndef generate_data(n: int):\n    systems = [\"ALITPC\", \"ALITRD\", \"ALITOF\", \"ATLRPC\", \"ATLTGC\", \n        \"ATLMDT\", \"CMSDT\", \"CMSRPC\", \"CMSCSC\", \"LHBRI1\", \"LHBRI2\", \"LHBMWP\"]\n    categories = [\"unProcessAlarm\", \"unProcessWarning\", \"PLCAlarm\", \"PGSAlarm\"]\n    messages = [\"Value outside range\", \"Bad Communication\", \"Connection issues\"]\n    now = datetime.now()\n    last_year = now - timedelta(days=365)\n    random_data = []\n    for i in range(n):\n        random_system = random.choice(systems)\n        random_category = random.choice(categories)\n        random_epoch = random.randrange(\n            int(last_year.timestamp()), int(now.timestamp())\n        )\n        random_timestamp = datetime.fromtimestamp(random_epoch)\n        random_messages = random.choice(messages)\n        random_text = f\"\"\"{random_timestamp} - {random_system} - {random_category} - {random_messages}\"\"\"\n        random_data.append(\n            (random_system, random_timestamp, random_category, random_text)\n        )\n    return random_data\n\nThis function is randomly sampling from a sequence of items to create unique rows. Also, a random timestamp is created by selecting a random value of epoch seconds between the current timestamp and the timestamp - 365 days.\nWe can then make a small function to insert the data in the database\n\n\napp/db.py\n\n...\n\ndef insert_data(db: str, table: str, data: list[tuple]):\n    \"\"\"Insert data in the table\"\"\"\n    conn = sqlite3.connect(db)\n    cur = conn.cursor()\n    cur.executemany(f\"INSERT INTO alarms VALUES (NULL, ?, ?, ?, ?)\", data)\n    conn.commit()\n    cur.close()\n    conn.close()\n\nNote that I have placed NULL because I let the id column autoincrement by itself.\nFinally, we can put the pieces together and run all the functions:\n\n\napp/db.py\n\n...\n\nif __name__ == \"__main__\":\n    random_data = generate_data(n=10_000)\n    create_db(db=\"db.sqlite\", table=\"alarms\")\n    insert_data(db=\"db.sqlite\", table=\"alarms\", data=random_data)\n\nThe database can be filled by calling the db.py file:\ncd app && python db.py\nWe can verify that the alarms table is filled by running a simple query using the sqlite3 cli:\n$ sqlite3 db.sqlite -cmd \".headers on\" \"SELECT * FROM alarms LIMIT 5\"\nid|system|timestamp|category|text\n1|ATLTGC|2023-02-19 10:05:04|unProcessWarning|2023-02-19 10:05:04 - ATLTGC - unProcessWarning - Connection issues\n2|LHBRI2|2023-01-18 19:44:03|unProcessWarning|2023-01-18 19:44:03 - LHBRI2 - unProcessWarning - Value outside range\n3|ALITPC|2023-07-17 00:46:08|PLCAlarm|2023-07-17 00:46:08 - ALITPC - PLCAlarm - Bad Communication\n4|CMSCSC|2023-01-18 03:57:57|unProcessWarning|2023-01-18 03:57:57 - CMSCSC - unProcessWarning - Bad Communication\n5|CMSRPC|2022-12-05 04:28:17|PLCAlarm|2022-12-05 04:28:17 - CMSRPC - PLCAlarm - Value outside range"
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#sharing-the-app-settings-and-database-connection",
    "href": "posts/tabulator-sql-fastapi/index.html#sharing-the-app-settings-and-database-connection",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "Sharing the app settings and database connection",
    "text": "Sharing the app settings and database connection\nBefore implementing the endpoints, it should be noted that the database could be accessed by different users and through different sessions. We could create a single session for the db that will get reused by the different server connections. In FastAPI, one can do this through its dependency injection system. First, we can create a class representing the settings to connect to the db:\n\n\napp/main.py\n\nfrom fastapi import FastAPI, Request, Query, Depends\nfrom typing import Annotated\nimport sqlite3\nfrom pydantic_settings import BaseSettings, SettingsConfigDict \nfrom pathlib import Path\n\n\nclass Settings(BaseSettings): \n    model_config = SettingsConfigDict(env_file=\".env\") \n    db_path: Path = 'app/db.sqlite' \n\n\nasync def get_settings():\n    yield Settings()\n\n\nasync def get_db(settings: Annotated[Settings, Depends(get_settings)]): \n    conn = sqlite3.connect(settings.db_path) \n    try: \n        yield conn \n    finally: \n        conn.close() \n\nThis class is based on pydantics and will allow to read the database from an .env file or environmental variable DB_PATH. By default we put db.sqlite, but pydantic will also check that the path is a valid path since we defined its type to be a python’s pathlib.Path.\nThen, we can create a dependency function that will yield our settings. This is the mechanism that FastAPI uses to defined dependencies that will be reused throughout the application.\n\n\napp/main.py\n\nfrom fastapi import FastAPI, Request, Query, Depends\nfrom typing import Annotated\nimport sqlite3\nfrom pydantic_settings import BaseSettings, SettingsConfigDict \nfrom pathlib import Path\n\n\nclass Settings(BaseSettings): \n    model_config = SettingsConfigDict(env_file=\".env\") \n    db_path: Path = 'app/db.sqlite'\n\n\nasync def get_settings(): \n    yield Settings() \n\n\nasync def get_db(settings: Annotated[Settings, Depends(get_settings)]): \n    conn = sqlite3.connect(settings.db_path) \n    try: \n        yield conn \n    finally: \n        conn.close() \n\nFinally, we create another dependency, which will return the connection object to the database:\n\n\napp/main.py\n\nfrom fastapi import FastAPI, Request, Query, Depends\nfrom typing import Annotated\nimport sqlite3\nfrom pydantic_settings import BaseSettings, SettingsConfigDict \nfrom pathlib import Path\n\n\nclass Settings(BaseSettings): \n    model_config = SettingsConfigDict(env_file=\".env\") \n    db_path: Path = 'app/db.sqlite'\n\n\nasync def get_settings():\n    yield Settings()\n\n\nasync def get_db(settings: Annotated[Settings, Depends(get_settings)]): \n    conn = sqlite3.connect(settings.db_path) \n    try: \n        yield conn \n    finally: \n        conn.close() \n\nYou can read more about dependency injection on FastAPI documentation but from my understanding the get_db is a dependency that requires another dependency and declares it by using the Depends function."
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#implementing-the-endpoints",
    "href": "posts/tabulator-sql-fastapi/index.html#implementing-the-endpoints",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "Implementing the endpoints",
    "text": "Implementing the endpoints\nWe now want to be able to fetch the database rows from the /alarm endpoint and the /alarms one. The /alarm endpoint should have a mandatory query parameter which will be the id. The /alarms should instead support pagination, i.e. it should be able to receive two parameters page, which should indicate which page we want to retrieve and size which indicates the amount of rows per page.\nIn the main.py, the endpoint for /alarm would look like this:\n\n\napp/main.py\n\n1@app.get(\"/alarm\")\nasync def alarm(\n    request: Request, \n2    id: Annotated[int, Query()],\n3    db: sqlite3.Connection = Depends(get_db),\n):\n4    db.row_factory = sqlite3.Row\n    cursor = db.cursor()\n5    cursor.execute(\"SELECT * FROM alarms WHERE id = ?\", (id, ))\n    result = cursor.fetchone()\n    return result\n\n\n1\n\nI defined a GET endpoint for /alarms\n\n2\n\nI’m telling FastAPI that id should be an int and a query parameter (passed through ‘?id=…’ on the url)\n\n3\n\nThe db object is the dependency that I have explained earlier\n\n4\n\nBy default the rows will be returned as a list of tuples but I wanted to return a list of dict where the keys are the column names.\n\n5\n\nI am passing a query, selecting by id and using a placeholder to bind external values\n\n\nMoving now to the /alarms endpoint, this will be a bit more sophisticated: in order to prepare the data to be consumed by Tabulator we would need to accept the size of a page size, the number of the page we want to get page, and we should return a json with one key called last_page indicating the total number of pages and one key called data with the returned data. The code looks like this:\n\n\napp/main.py\n\n@app.get(\"/alarms\")\nasync def home(\n    request: Request,\n1    page: Annotated[int, Query(ge=1)] = 1,\n    size: Annotated[int, Query(lt=100)] = 100,\n    db: sqlite3.Connection = Depends(get_db) \n):\n    db.row_factory = sqlite3.Row\n    cursor = db.cursor()\n2    offset = (page - 1) * size\n3    cursor.execute(\"SELECT COUNT(id) FROM alarms\")\n    n_rows = cursor.fetchone()['count']\n4    n_pages = n_rows // size if n_rows % size == 0 else n_rows // size + 1\n    \n5    cursor.execute(\"SELECT * FROM alarms LIMIT ? OFFSET ?\", (size, offset))\n    alarms = cursor.fetchall()\n    return {\n        'last_page': n_pages,\n        'data': alarms \n    }\n\n\n1\n\npage is a query parameter from a GET request. I’ve put the constraint that it should be always greater or equal than 1 ge=1. Similarly, size is a query parameter and I’ve put a constraint on the maximum number of items that can be requested\n\n2\n\nI’m computing an offset that will be used by the SQL query to get the paginated rows\n\n3\n\nTabulator needs the total number of page so we need to count how many rows we have in the table\n\n4\n\nThe number of pages depends on the number of rows and on the size of the page. Remember to add one for the remainder rows from the division n_rows/size\n\n5\n\nI’m selecting the alarms with size and offset\n\n\nIf you look now what you get at localhost:8000/alarms you would see something like:\n// http://127.0.0.1:8000/alarms\n\n{\n  \"last_page\": 100,\n  \"data\": [\n    [\n      1,\n      \"CMSCSC\",\n      \"2023-08-28 12:02:43\",\n      \"unProcessWarning\",\n      \"2023-08-28 12:02:43 - CMSCSC - unProcessWarning - Bad Communication\"\n    ],\n    // ...\n}"
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#setting-up-tabulator",
    "href": "posts/tabulator-sql-fastapi/index.html#setting-up-tabulator",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "Setting up tabulator",
    "text": "Setting up tabulator\nNow we can create a home page and setting up tabulator. We can start by defining an index.html file which will be returned by FastAPI when visiting /. We can create the file inside a templates folder and serve it as a jinja template in case we would need to add some custom logic later.\n\nCreate the templates folder:\n\nmkdir app/templates\n\nAdd and index.html file inside the folder:\n\n\n\napp/templates/index.html\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Alarms&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n            background-color: #f0f0f0;\n        }\n        h1 {\n            color: #333;\n            text-align: center;\n            padding: 20px;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Alarms list&lt;/h1&gt;\n    &lt;div id=\"alarms-table\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nI have added a bare minimum amount of CSS to have a gray background and the title centered. We can now serve the file from FastAPI. To do so, we need to change the main.py file\n\n\napp/main.py\n\nfrom fastapi import FastAPI, Request, Query, Depends, Body\nfrom fastapi.templating import Jinja2Templates \nfrom typing import Annotated\nimport sqlite3\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom pathlib import Path\n\n\nclass Settings(BaseSettings):\n    model_config = SettingsConfigDict(env_file=\".env\")\n    db_path: Path = \"app/db.sqlite\"\n\n\nasync def get_settings():\n    yield Settings()\n\n\nasync def get_db(settings: Annotated[Settings, Depends(get_settings)]):\n    conn = sqlite3.connect(settings.db_path)\n    try:\n        yield conn\n    finally:\n        conn.close()\n\n\napp = FastAPI()\ntemplates = Jinja2Templates('app/templates') \n\n\n@app.get(\"/\") \nasync def home(request: Request): \n    return templates.TemplateResponse('index.html', context={'request': request}) \n\n...\n\nHere we created a templates object that is using Jinja2 to render the files inside the app/templates folder. Note that for each TemplateResponse you need to pass a context object with the request.\nWe can now work on the index.html file by importing tabulator from a CDN and creating the Javascript object by refering to the documentation on loading AJAX data and on setting server side pagination:\n\n\napp/index.html\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Alarms&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n            background-color: #f0f0f0;\n        }\n        h1 {\n            color: #333;\n            text-align: center;\n            padding: 20px;\n        }\n        #alarms-table {\n            margin: 3rem;\n        }\n    &lt;/style&gt;\n    &lt;link href=\"https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n    &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Alarms list&lt;/h1&gt;\n    &lt;div id=\"alarms-table\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n        var table = new Tabulator(\"#alarms-table\", {\n            layout: 'fitDataStretch',\n            ajaxURL: \"/alarms\",\n            ajaxContentType: 'json',\n            ajaxConfig: 'GET',\n            pagination: true,\n            paginationMode: \"remote\",\n            autoColumns: true, \n            paginationSize: 10 \n        })\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nFew things to note here:\n\nPagination must be enabled and set to remote. If not, tabulator will expect the ajaxURL to provide an array of records.\nSetting autoColumns is needed if you don’t want to define the column by yourself.\n\nThis is what my web page look like:\n\n\n\nHome page with tabulator loading data from a DB"
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#adding-server-side-filtering",
    "href": "posts/tabulator-sql-fastapi/index.html#adding-server-side-filtering",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "Adding server side filtering",
    "text": "Adding server side filtering\nTabulator allows to have some filters on the header of each column. A query is sent to the ajaxURL in the form of an array. We should also change the method from GET to POST, as I had a hard-time trying to pass URL-encoded lists to FastAPI and get it parsed as query parameters, so a body parameter is more adequate. Let’s enable header filters:\n\n\napp/index.html\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Alarms&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n            background-color: #f0f0f0;\n        }\n        h1 {\n            color: #333;\n            text-align: center;\n            padding: 20px;\n        }\n        #alarms-table {\n            margin: 3rem;\n        }\n    &lt;/style&gt;\n    &lt;link href=\"https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n    &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Alarms list&lt;/h1&gt;\n    &lt;div id=\"alarms-table\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n        var table = new Tabulator(\"#alarms-table\", {\n            layout: 'fitDataStretch',\n            ajaxURL: \"/alarms\",\n            ajaxContentType: 'json',\n            ajaxConfig: 'POST',\n            pagination: true,\n            paginationMode: \"remote\",\n            paginationSize: 10,\n            filterMode: 'remote',\n            autoColumns: true,\n            autoColumnsDefinitions: [\n                {field: 'text', title: 'Text', headerFilter: \"input\"}\n            ]\n        })\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nAnd update the code in the main.py file\n\n\napp/main.py\n\nfrom fastapi import FastAPI, Request, Query, Depends, Body \n\n...\n\n@app.post(\"/alarms\") \nasync def home(\n    request: Request,\n    page: Annotated[int, Body(ge=1)] = 1, \n    size: Annotated[int, Body(lt=100)] = 100, \n    db: sqlite3.Connection = Depends(get_db), \n    filter: Annotated[list[dict], Body()] = None \n):\n    db.row_factory = sqlite3.Row\n    cursor = db.cursor()\n    offset = (page - 1) * size\n    cursor.execute(\"SELECT COUNT(id) as count FROM alarms\")\n    n_rows = cursor.fetchone()['count']\n    n_pages = n_rows // size if n_rows % size == 0 else n_rows // size + 1\n    \n    cursor.execute(\"SELECT * FROM alarms LIMIT ? OFFSET ?\", (size, offset))\n    alarms = cursor.fetchall()\n    return {\n        'last_page': n_pages,\n        'data': alarms \n    }\n\nIf you inspect the request from the developer console, you would see that when typing something in the text this json payload is sent:\n{\"filter\":[{\"field\":\"text\",\"type\":\"like\",\"value\":\"rpc\"}],\"page\":1,\"size\":10}\nThe filter key contains a list of filters, each one for a set of condition that we can use to filter the data on our db. In particular, the field key refers to the column of the table, the type is the operator used and value is the value typed in in the header input. We should only be careful about possible SQL injections. For this reason\n\n\napp/main.py\n\n@app.post(\"/alarms\")\nasync def home(\n    request: Request,\n    page: Annotated[int, Body(ge=1)] = 1,\n    size: Annotated[int, Body(lt=100)] = 100,\n    db: sqlite3.Connection = Depends(get_db), \n    filter: Annotated[list[dict], Body()] = None\n):\n    db.row_factory = sqlite3.Row\n    cursor = db.cursor()\n1    where_clauses = []\n    where_query = \"\"\n    placeholder_values = []\n    if len(filter):  # Apply a WHERE clause in the SQL query\n        where_query += \" WHERE \"\n        for filter_obj in filter:\n            field, type, value = filter_obj['field'], filter_obj['type'], filter_obj['value']\n            # Check that the filter field can be trusted and exists as a column in our table\n2            cursor.execute(\"select name from PRAGMA_TABLE_INFO('alarms') where name = ?\", (field,))\n            column_field = cursor.fetchone()\n            operand = type.upper()\n            if column_field:  # the filter field exists as a column\n3                match operand:\n                    case 'LIKE': # if using like as tabulator mention, add wildcards\n                        placeholder_value = f'%{value}%'\n                    case '=':  # this is mostly used for numerical, categorical or date filtering\n                        placeholder_value = value\n                    case other:  # the operand is not supported, return an HTTP 400 response\n                        raise HTTPException(status_code=400, detail=f\"Filter type {operand} not supported.\")\n    \n                where_clauses.append(f\"{column_field['name']} {operand} ?\")\n                placeholder_values.append(placeholder_value)\n        where_query += \" AND \".join(where_clauses)\n                    \n    offset = (page - 1) * size\n    cursor.execute(\"SELECT COUNT(id) as count FROM alarms\")\n    n_rows = cursor.fetchone()['count']\n    n_pages = n_rows // size if n_rows % size == 0 else n_rows // size + 1\n    \n    cursor.execute(f\"SELECT * FROM alarms {where_query} LIMIT ? OFFSET ?\", (*placeholder_values, size, offset))\n    alarms = cursor.fetchall()\n    return {\n        'last_page': n_pages,\n        'data': alarms \n    }\n\n\n1\n\nI’m converting the list of filters as a list of SQL expressions that will end inside a WHERE clause.\n\n2\n\nIn this case I’m using an sqlite function to check if the field in the filter object passed is actually an existing column. In other databases and libraries such as PostgreSQL and pyscopg you can safely escape SQL identifier. In this case you can’t so either you escape the column name by your own or (at least I think) you check that the column name is valid and existing.\n\n3\n\nThe safety on the user input applies also for the operand, called type in the filter payload. In this case I’m supporting only two operators: LIKE and =. Any other string would raise an HTTP response with status code 400\n\n\nAnd that’s it, now we have text filtering!"
  },
  {
    "objectID": "posts/tabulator-sql-fastapi/index.html#adding-date-filters",
    "href": "posts/tabulator-sql-fastapi/index.html#adding-date-filters",
    "title": "Display data from a SQL source on the web with Tabulator and FastAPI",
    "section": "Adding date filters",
    "text": "Adding date filters\nNow that the general idea behind server side filtering is clear, we could extend the filtering also to dates.\n\n\napp/templates/index.html\n\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Alarms&lt;/title&gt;\n    &lt;style&gt;\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 0;\n            background-color: #f0f0f0;\n        }\n        h1 {\n            color: #333;\n            text-align: center;\n            padding: 20px;\n        }\n        #alarms-table {\n            margin: 3rem;\n        }\n    &lt;/style&gt;\n    &lt;link href=\"https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator.min.css\" rel=\"stylesheet\"&gt;\n    &lt;script type=\"text/javascript\" src=\"https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/luxon/3.4.3/luxon.min.js\"\n        integrity=\"sha512-gUQcFuEaDuAEqvxIQ9GDdMcCeFmG5MPnoc6ruJn+nyCNHrHM2oB97GOVLIOiixzTxPYmIfEQbOoQmx55UscLyw==\"\n        crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Alarms list&lt;/h1&gt;\n    &lt;div id=\"alarms-table\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n        var table = new Tabulator(\"#alarms-table\", {\n            layout: 'fitDataStretch',\n            ajaxURL: \"/alarms\",\n            ajaxContentType: 'json',\n            ajaxConfig: 'POST',\n            pagination: true,\n            paginationMode: \"remote\",\n            paginationSize: 10,\n            filterMode: 'remote',\n            autoColumns: true,\n            autoColumnsDefinitions: [\n                {field: 'text', title: 'Text', headerFilter: \"input\"},\n                {field: 'timestamp', title: 'Alarm timestamp', \n                 formatter: 'datetime', headerFilter: 'date'}\n            ]\n        })\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nNote that I have added the luxon.js library as mentioned in the tabulator documentation. I have a put a date filter for a datetime column. This is because I would like the filter to get only the rows received on a particular date.\nOn the backend side we would need to deal with this particular case: if the column is timestamp, then we can convert the timestamp to date and use it for our where clause:\n\n\napp/main.py\n\n...\n\n@app.post(\"/alarms\")\nasync def home(\n    request: Request,\n    page: Annotated[int, Body(ge=1)] = 1,\n    size: Annotated[int, Body(lt=100)] = 100,\n    db: sqlite3.Connection = Depends(get_db), \n    filter: Annotated[list[dict], Body()] = None\n):\n    db.row_factory = sqlite3.Row\n    cursor = db.cursor()\n    where_clauses = []\n    where_query = \"\"\n    placeholder_values = []\n    if len(filter):  # Apply a WHERE clause in the SQL query\n        where_query += \" WHERE \"\n        for filter_obj in filter:\n            field, type, value = filter_obj['field'], filter_obj['type'], filter_obj['value']\n            # Check that the filter field can be trusted and exists as a column in our table\n            cursor.execute(\"select name from PRAGMA_TABLE_INFO('alarms') where name = ?\", (field,))\n            column_field = cursor.fetchone()\n            operand = type.upper()\n            if column_field:  # the filter field exists as a column\n                identifier = column_field['name'] \n                match (operand, identifier): \n                    case 'LIKE', _: # if using like as tabulator mention, add wildcards \n                        placeholder_value = f'%{value}%' \n                    case ('=', 'timestamp'): \n                        # convert the timestamp identifier column to a date \n                        column_field \n                        placeholder_value = value \n                        identifier = f\"strftime('%Y-%m-%d', {column_field['name']})\" \n                    case ('=', _):  # this is mostly used for numerical, categorical or date filtering \n                        placeholder_value = value \n\n                    case other:  # the operand is not supported, return an HTTP 400 response\n                        raise HTTPException(status_code=400, detail=f\"Filter type {operand} not supported.\")\n    \n                where_clauses.append(f\"{identifier} {operand} ?\")\n                placeholder_values.append(placeholder_value)\n        where_query += \" AND \".join(where_clauses)\n                    \n    offset = (page - 1) * size\n    cursor.execute(\"SELECT COUNT(id) as count FROM alarms\")\n    n_rows = cursor.fetchone()['count']\n    n_pages = n_rows // size if n_rows % size == 0 else n_rows // size + 1\n    \n    cursor.execute(f\"SELECT * FROM alarms {where_query} LIMIT ? OFFSET ?\", (*placeholder_values, size, offset))\n    alarms = cursor.fetchall()\n    return {\n        'last_page': n_pages,\n        'data': alarms \n    }\n\nAs you can see I made use of python’s match operator to match on both the field and the type. In the case of alarm as a field I am casting the datetime column to date.\nAfter setting all this up you should have a nice table that allows you to filter on the text and date column.\nThis is the end result I’m getting for the code written:\n\n\nVideo\nTabulator with server side filtering"
  },
  {
    "objectID": "posts/time-over-threshold-in-signals/index.html",
    "href": "posts/time-over-threshold-in-signals/index.html",
    "title": "Time Over Threshold in signal processing",
    "section": "",
    "text": "A part of my work consists of acquiring the signal induced on a set of copper strips of a gaseous detectors. The signals dynamics such as the height, shape, duration, typically give useful information about what is going on inside the detector. The signal generated by a gasesous detector depends on several factors: - he geometry, layout and working principle of the detector itself; - the electric field applied in the gas medium that allows for the electron created by an ionizing particle to start an avalanche process and move a sufficiently high number of electrons to be detected by the electronics; - the gas used, which greatly affects the dynamic of the avalanche development\nAmong all of these parameters, I would like to focus on one: the time over threshold. The time over threshold can be defined in different ways. For simplicity, I will define it as the difference of the time between the signal first and last crosses a fixed treshold.\nFor example, look at the plot Figure 1\n\n\nCode\n# The holy trininty of libraries that will be used\n# throughout the post\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Just to set the maximum of items to be printed for an array\nnp.set_printoptions(threshold=5)\n\n# A better style for plots\nplt.style.use('https://gitlab.cern.ch/-/snippets/2223/raw/master/rpcecogas.mplstyle')\n\n\n\n\nCode\nwindow_length = 520\n# This file contains 10 waveforms of 520 samples. Each sample is written on a single line\ndf = pd.read_csv(\"https://gist.githubusercontent.com/grigolet/09ffada96abb2acf0fe34070f0e83211/raw/3880aa89e4b113ea2866ec8e3768c03e98f84118/signals.txt\",\n                header=None, names=['waveform'])\n# in the next line I will transform the dataframe into a 2d\n# numpy array with shape (number of signals, window length).\nwaveforms = df.waveform.values.reshape((-1, window_length))\n\nfig, ax = plt.subplots()\nax.plot(waveforms.T);\nax.set(xlabel='Time [2 ns]', ylabel='ADC [a.u.]', title='Waveforms');\n\n\n\n\n\nFigure 1: Waveform read-out from a gaseous detector\n\n\n\n\nImage that we would like to compute the duration of all the signals below a threshold that I arbitrarly fix. For example:\n\n\nCode\n# Prepare and read the data\nwindow_length = 520\nthreshold = 8235\n# This file contains 10 waveforms of 520 samples. Each sample is written on a single line\ndf = pd.read_csv(\"https://gist.githubusercontent.com/grigolet/09ffada96abb2acf0fe34070f0e83211/raw/3880aa89e4b113ea2866ec8e3768c03e98f84118/signals.txt\",\n                header=None, names=['waveform'])\n# in the next line I will transform the dataframe into a 2d\n# numpy array with shape (number of signals, window length).\nwaveforms = df.waveform.values.reshape((-1, window_length))\n\nfig, ax = plt.subplots()\nfor ix, waveform in enumerate(waveforms):\n    ax.plot(waveform, label=f\"Waveform {ix}\")\nax.axhline(threshold, color='r', linestyle='--')\nax.legend(loc='upper left', bbox_to_anchor=(1, 1))\nax.set(xlabel='Time [2 ns]', ylabel='ADC [a.u.]', title=f'Waveforms. Threshold at {threshold}');"
  },
  {
    "objectID": "posts/time-over-threshold-in-signals/index.html#the-1d-case",
    "href": "posts/time-over-threshold-in-signals/index.html#the-1d-case",
    "title": "Time Over Threshold in signal processing",
    "section": "The 1d case",
    "text": "The 1d case\nIn this case, the easiest and probably more natual approach to compute the ToT would be to get the first sample below the threshold, the last sample below the threshold and do a difference.\nThis is fairly straightforward in the case of a single signal and it can be accomplished in few different ways:\n\n\nCode\n# From the previous cell we have waveforms available in a 2d\n# numpy array of shape (n waveforms, n samples)\n# I will take waveform 8 just for example purposes\n\nthreshold = 8235\nwaveform = waveforms[8, :]\nfig, ax = plt.subplots()\nax.plot(waveform)\nax.axhline(threshold, color='r', linestyle='--')\n\n\n&lt;matplotlib.lines.Line2D at 0x7fb662c0cd90&gt;\n\n\n\n\n\nFor the way I defined the time over threshold, I would have to take the first sample before crossing the threshold and the last one before going back\n\nsamples_below_threshold = waveform &lt; threshold\n# Boolean array indicating which sample is below the threshold\nsamples_below_threshold\n\narray([False, False, False, ..., False, False, False])\n\n\nIf I imagine the boolean array defined above as an integer array, I could use np.argmax() to find the first item for which the condition is true, i.e. the first item crossing the threshold.\nTo find instead the last item I could use np.argmax() but applied to the reversed array, taking into account that the resulting index should be then subtracted with the number of samples of the waveform\n\nfirst_item = np.argmax(samples_below_threshold)\nlast_item = window_length - np.argmax(samples_below_threshold[::-1]) -1\nfirst_item, last_item\n\nfig, ax = plt.subplots()\nax.plot(waveform)\nax.axhline(threshold, color='r', linestyle='--')\nax.axvline(first_item, color='r')\nax.axvline(last_item, color='r')\n\n&lt;matplotlib.lines.Line2D at 0x7fb662bb13f0&gt;"
  },
  {
    "objectID": "posts/time-over-threshold-in-signals/index.html#the-2d-case",
    "href": "posts/time-over-threshold-in-signals/index.html#the-2d-case",
    "title": "Time Over Threshold in signal processing",
    "section": "The 2d case",
    "text": "The 2d case\nIn a case of a 2d array like waveforms, some numpy gym can be used to perform vectorized operation and avoid using for loops.\nA trick I often used to remember on which axis I want to perform an operation on a numpy.ndarray: if the numpy array has a shape (x_dim, y_dim, z_dim, …) then the operation I perform the aggregation on is making that axis collapse.\nSo in the present case, if I have an (n_waveforms, n_samples) array, then I would expect to have the argmax() of each waveform, so an array with a shape (n_waveforms). This means that axis=0 should be preversed and axis=1 should collapse, so the np.argmax() operation should have axis=1.\nIt’s a bit tricky but it’s working fine in my mental model\n\nsamples_below_threshold = waveforms &lt; threshold\n# Now we have a 2d array of waveforms but\n# an analogue boolean mask\nsamples_below_threshold\n\nfirst_items = np.argmax(samples_below_threshold, axis=1)\n# Reverse the array over the samples dimensions to get the last \n# point of threshold crossing\nlast_items = window_length - np.argmax(samples_below_threshold[:, ::-1], axis=1) - 1\n\nprint(first_items, last_items)\n\n# Let's graphically check the results\nfig, ax = plt.subplots()\nfor ix, waveform in enumerate(waveforms):\n    ax.plot(waveform, label=f\"Waveform {ix}\")\n    ax.plot(first_items[ix], threshold, f'C{ix}.')\n    ax.plot(last_items[ix], threshold, f'C{ix}.')\nax.legend(loc='upper left', bbox_to_anchor=(1, 1))\nax.set(xlabel='Time [2 ns]', ylabel='ADC [a.u.]', title=f'Waveforms. Threshold at {threshold}');\n\n[227   0 232 ... 236 249   0] [230 519 237 ... 240 346 519]\n\n\n\n\n\nIn the example above I marked the points at which the threshold is crossed. However, in the case of a waveform not crossing the threshold, the np.argmax() function would return 0, so first_item would be equal to 0 and last_item would be equal to window_length - 1, as it is for the 9th waveform:\n\nn_waveform = 9\nwaveform = waveforms[n_waveform, :]\nfig, ax = plt.subplots()\n\nprint(first_items[n_waveform], last_items[n_waveform])\nax.plot(waveform)\nax.axhline(threshold, color='r', linestyle='--')\n\n0 519\n\n\n&lt;matplotlib.lines.Line2D at 0x7fb66347db40&gt;\n\n\n\n\n\nSo at this step the time over threshold can be simply computed as last_items - first_items, discarding the values equal to window_length - 1\n\ntime_over_threshold = last_items - first_items\ntime_over_threshold[time_over_threshold &lt; (window_length  - 1)]\n\narray([ 3,  5,  1, 69,  4, 97])"
  },
  {
    "objectID": "posts/time-over-threshold-in-signals/index.html#the-1d-case-1",
    "href": "posts/time-over-threshold-in-signals/index.html#the-1d-case-1",
    "title": "Time Over Threshold in signal processing",
    "section": "The 1d case",
    "text": "The 1d case\nIn this case I could set up the code in this way:\n\nFind the coordinates before meeting the thresholds, i.e. \\((x_{fb}, y_{fb})\\) and \\((x_{lb}, y_{lb})\\)\nFind the coordinates after meeting the thresholds, i.e. \\((x_{fa}, y_{fa})\\) and \\((x_{la}, y_{la})\\)\nFind \\(x_f\\) using some simple line equation: \\[\n\\frac{x_f- x_{fb}}{y_t - y_{fb}} = \\frac{x_{fa} - x_{fb}}{y_{fa} - y_{fb}}\n\\] \\[\nx_f = x_{fb} + (x_{fa} - x_{fb}) \\left(\\frac{y_t - y_{fb}}{y_{fa} - y_{fb}}\\right)\n\\] In a similar way, for \\(x_l\\): \\[\nx_l = x_{lb} + (x_{la} - x_{lb}) \\left(\\frac{y_t - y_{fb}}{y_{la} - y_{lb}}\\right)\n\\]\nCalculate the time over threshold as \\(x_l - x_f\\)\n\n\n# I'm taking the threshold as before as an example\nwaveform = waveforms[8, :]\nsamples_below_threshold = waveform &lt; threshold\n# Find the indices xfb and xlb\nxfb = np.argmax(samples_below_threshold) - 1\nxlb = window_length - np.argmax(samples_below_threshold[::-1]) - 2\n# The corresponding yfb and ylb are easy to get:\nyfb, ylb = waveform[xfb], waveform[xlb]\n# Now we can also find the coordinates of the samples\n# after meeting the threshold\nxfa, yfa = xfb + 1, waveform[xfb + 1]\nxla, yla = xlb + 1, waveform[xlb + 1]\n# Find xf and xl\nxf = xfb + (xfa - xfb) * ((threshold - yfb) / (yfa - yfb))\nxl = xlb + (xla - xlb) * ((threshold - ylb) / (yla - ylb))\ntime_over_threshold = xl - xf\n\n# Let's visualize everything\nfig, ax = plt.subplots()\nax.plot(waveform, '.-')\nax.axvline(xfb, color='C3')\nax.axvline(xf, color='C2', linestyle='--')\nax.axvline(xfa, color='C3')\n\nax.axvline(xlb, color='C3')\nax.axvline(xl, color='C2', linestyle='--')\nax.axvline(xla, color='C3')\n\nax.axhline(threshold, color='C1', linestyle='--')\n\n# zoom in in the region of interest\nax.set(xlim=(xfb - 5, xla + 5), title=f'Time over threshold = {time_over_threshold:.1f}')\n\n[(243.0, 351.0), Text(0.5, 1.0, 'Time over threshold = 96.4')]\n\n\n\n\n\nAs expected, the time over threshold is now computed as the intersection between the horizontal lines and the segments first and last crossing the threshold. The code above is failing in case the threshold is not crossing any segment:\n\n# Waveform 9 is not crossing the threshold\nwaveform = waveforms[9, :]\nsamples_below_threshold = waveform &lt; threshold\n# np.argmax returns 0 if nothing is found\nxfb = np.argmax(samples_below_threshold) - 1\n# This would generated some issues because it's 520 - 0 = 520 \nxlb = window_length - np.argmax(samples_below_threshold[::-1]) \n# This part will throw an error because waveform has only\n# 520 samples and waveform[520] is outside range\nyfb, ylb = waveform[xfb], waveform[xlb]\n# ...\n\nIndexError: index 520 is out of bounds for axis 0 with size 520\n\n\nThe code is failing because it’s not possible to find the x coordinates for the segment last crossing the threshold. This results in a value of \\(x_{lb}\\) of 520 that is outside the range of the array (which starts from 0 to 519). A possible solution could be to set the values of \\(x_{lb}\\) to be 0 or np.nan in case they are found to be 520. This would result in a time over threshold of 0. This is the cleaniest solution I could find:\n\nwaveform = waveforms[9, :]\nsamples_below_threshold = waveform &lt; threshold\nxfb = np.argmax(samples_below_threshold) - 1\n# Set xfb to be nan in case no threshold crossing\n# is found. The same for xlb\nxfb = np.where(xfb != -1, xfb, 0)\nxlb = window_length - np.argmax(samples_below_threshold[::-1]) \nxlb = np.where(xlb != window_length, xlb, 0)\n# The corresponding yfb and ylb are easy to get:\nyfb, ylb = waveform[xfb], waveform[xlb]\n# ... time over threshold will result in a 0 value"
  },
  {
    "objectID": "posts/time-over-threshold-in-signals/index.html#the-2d-case-1",
    "href": "posts/time-over-threshold-in-signals/index.html#the-2d-case-1",
    "title": "Time Over Threshold in signal processing",
    "section": "The 2d case",
    "text": "The 2d case\nThe 2d case is a bit more complicated as it involves the usual numpy model to solve this problem in an efficient way. Nevertheless, the steps to approach the problem are the same so the code looks like the following:\n\nsamples_below_threshold = waveforms &lt; threshold\n# Find the indices xfb and xlb\nxfb = np.argmax(samples_below_threshold, axis=1) - 1\n# Add np.where condition to handle signal without threshold crossing\nxfb = np.where(xfb != -1, xfb, 0)\nxlb = window_length - np.argmax(samples_below_threshold[:, ::-1], axis=1) - 2\nxlb = np.where(xlb != window_length, xlb, 0)\n# Taking indices along a 2d array is a bit\n# more complicated. There are few different\n# ways to do it\nyfb = np.take_along_axis(waveforms, xfb[:, None], axis=1).flatten()\nylb = np.take_along_axis(waveforms, xlb[:, None], axis=1).flatten()\n\nxfa = xfb + 1\nyfa = np.take_along_axis(waveforms, xfb[:, None] + 1, axis=1).flatten()\nxla = xlb + 1\nyla = np.take_along_axis(waveforms, xlb[:, None] + 1, axis=1).flatten()\n\nxf = xfb + (xfa - xfb) * ((threshold - yfb) / (yfa - yfb))\nxl = xlb + (xla - xlb) * ((threshold - ylb) / (yla - ylb))\ntime_over_threshold = xl - xf\n\n# Let's visualize everything\nfig, axs = plt.subplots(3, 4, figsize=(4*4, 4*3))\nfor ix, waveform in enumerate(waveforms):\n    ax = axs.flat[ix]\n    ax.plot(waveform, '.-')\n    ax.axvline(xfb[ix], color='C3')\n    ax.axvline(xf[ix], color='C2', linestyle='--')\n    ax.axvline(xfa[ix], color='C3')\n\n    ax.axvline(xlb[ix], color='C3')\n    ax.axvline(xl[ix], color='C2', linestyle='--')\n    ax.axvline(xla[ix], color='C3')\n\n    ax.axhline(threshold, color='C1', linestyle='--')\n\n    ax.set(xlim=(xfb[ix] - 5, xla[ix] + 5), \n    ylim=(threshold - 20, threshold + 20),\n    title=f'Waveform {ix}.\\nTime over threshold = {time_over_threshold[ix]:.1f} samples')\n    fig.tight_layout()\n\n/tmp/ipykernel_389329/2995661854.py:20: RuntimeWarning: divide by zero encountered in true_divide\n  xl = xlb + (xla - xlb) * ((threshold - ylb) / (yla - ylb))\n\n\n\n\n\nYou may notice that there are few corner cases that may arise. For instance, have a look at waveform 5: the resulting time over threshold is nan. In this case, if the segment defined by \\((x_{lb}, y_{lb}), (x_{la}, y_{la})\\) and crossing the threshold is parallel to the threshold itself, the values of either \\(y_{fa} - y_{fb}\\) or \\(y_{la} - y_{lb}\\) are equal to 0, resulting in an error similar to:\nRuntimeWarning: invalid value encountered in true_divide\n  xl = xlb + (xla - xlb) * ((threshold - ylb) / (yla - ylb))\nA possible solution to this is to handle the division using np.divide, which allows to use a where option and that allows to avoid to try/catch or use a context manager to handle the error. For more information see this Stack overflow post.\nThe code above will become the following:\n\nsamples_below_threshold = waveforms &lt; threshold\n# Find the indices xfb and xlb\nxfb = np.argmax(samples_below_threshold, axis=1) - 1\n# Add np.where condition to handle signal without threshold crossing\nxfb = np.where(xfb != -1, xfb, 0)\nxlb = window_length - np.argmax(samples_below_threshold[:, ::-1], axis=1) - 2\nxlb = np.where(xlb != window_length, xlb, 0)\n# Taking indices along a 2d array is a bit\n# more complicated. There are few different\n# ways to do it\nyfb = np.take_along_axis(waveforms, xfb[:, None], axis=1).flatten()\nylb = np.take_along_axis(waveforms, xlb[:, None], axis=1).flatten()\n\nxfa = xfb + 1\nyfa = np.take_along_axis(waveforms, xfb[:, None] + 1, axis=1).flatten()\nxla = xlb + 1\nyla = np.take_along_axis(waveforms, xlb[:, None] + 1, axis=1).flatten()\n\n# temp_yf and temp_yl are the arrays to use in case the 'where' condition is False. \n# If the 'where' condition is true, then the np.divide() result is used instead\ntemp_yf, temp_yl = np.zeros_like(yfa, dtype='float64'), np.zeros_like(ylb, dtype='float64')\nxf = xfb + (xfa - xfb) * (np.divide((threshold - yfb) , (yfa - yfb), out=temp_yf, where=(yfa - yfb) != 0))\nxl = xlb + (xla - xlb) * (np.divide((threshold - ylb) , (yla - ylb), out=temp_yl, where=(yla - ylb) != 0))\ntime_over_threshold = xl - xf\n\n# Let's visualize everything\nfig, axs = plt.subplots(3, 4, figsize=(4*4, 4*3))\nfor ix, waveform in enumerate(waveforms):\n    ax = axs.flat[ix]\n    ax.plot(waveform, '.-')\n    ax.axvline(xfb[ix], color='C3')\n    ax.axvline(xf[ix], color='C2', linestyle='--')\n    ax.axvline(xfa[ix], color='C3')\n\n    ax.axvline(xlb[ix], color='C3')\n    ax.axvline(xl[ix], color='C2', linestyle='--')\n    ax.axvline(xla[ix], color='C3')\n\n    ax.axhline(threshold, color='C1', linestyle='--')\n\n    ax.set(xlim=(xfb[ix] - 5, xla[ix] + 5), \n    ylim=(threshold - 20, threshold + 20),\n    title=f'Waveform {ix}.\\nTime over threshold = {time_over_threshold[ix]:.1f} samples')\n    fig.tight_layout()"
  },
  {
    "objectID": "posts/time-over-threshold-in-signals/index.html#handling-corner-cases",
    "href": "posts/time-over-threshold-in-signals/index.html#handling-corner-cases",
    "title": "Time Over Threshold in signal processing",
    "section": "Handling corner cases",
    "text": "Handling corner cases\nUnfortunately, the code below does not handle all the cases. Let’s wrap the previous code into a function and prepare some test cases\n\ndef compute_time_over_threshold(data: np.ndarray, threshold: float | int | np.ndarray) -&gt; np.ndarray: \n    \"\"\"Compute the time over threshold for a 1d or 2d array given a fixed threshold. The\n    time over threshold is defined as the difference between the intersection of the\n    threshold with the segments of the samples: the last - the first segments\n    crossing this threshold define the tot. In case no samples cross the\n    tot, the resulting value is 0.as_integer_ratio\n    \n    Parameters\n    ----------\n    data: ndarray\n        1d or 2d array of shape (n_waveforms, n_samples)\n    threshold: float or int or ndarray (n_waveforms)\n        a fixed threshold for all the waveforms or a set of threshold\n        for each waveform\n\n    Returns\n    -------\n    out: ndarray\n        a 1d array of the time over thresholds of shape (n_waveforms)\n    \"\"\"\n    # In case of a 1d array of shape (n_samples), convert it to a 2d\n    # array of shape (1, n_samples), so that the code can be used for\n    # both 1d and 2d array\n    if data.ndim == 1:\n        waveforms = data[None, :]\n    else:\n        waveforms = data\n    # The same operation for threshold:\n    if not isinstance(threshold, np.ndarray):\n        threshold = np.array([threshold])\n\n    window_length = waveforms.shape[1]\n    \n    samples_below_threshold = waveforms &lt; threshold[:, None]\n    # Find the indices xfb and xlb\n    xfb = np.argmax(samples_below_threshold, axis=1) - 1\n    # Add np.where condition to handle signal without threshold crossing\n    xfb = np.where(xfb != -1, xfb, 0)\n    xlb = window_length - np.argmax(samples_below_threshold[:, ::-1], axis=1) - 1\n    xlb = np.where(xlb != window_length - 1, xlb, 0)\n    # Taking indices along a 2d array is a bit\n    # more complicated. There are few different\n    # ways to do it\n    yfb = np.take_along_axis(waveforms, xfb[:, None], axis=1).flatten()\n    ylb = np.take_along_axis(waveforms, xlb[:, None], axis=1).flatten()\n\n    xfa = xfb + 1\n    yfa = np.take_along_axis(waveforms, xfb[:, None] + 1, axis=1).flatten()\n    xla = xlb + 1\n    yla = np.take_along_axis(waveforms, xlb[:, None] + 1, axis=1).flatten()\n\n    # temp_yf and temp_yl are the arrays to use in case the 'where' condition is False. \n    # If the 'where' condition is true, then the np.divide() result is used instead\n    temp_yf, temp_yl = np.zeros_like(yfa, dtype='float64'), np.zeros_like(ylb, dtype='float64')\n    xf = xfb + (xfa - xfb) * (np.divide((threshold - yfb) , (yfa - yfb), out=temp_yf, where=(yfa - yfb) != 0))\n    xl = xlb + (xla - xlb) * (np.divide((threshold - ylb) , (yla - ylb), out=temp_yl, where=(yla - ylb) != 0))\n    time_over_threshold = xl - xf\n\n    return time_over_threshold\n\nwaveform_test = np.array([-1, -1, -1, -1, -1])\nthreshold_test = -0.5\ntot = compute_time_over_threshold(waveform_test, threshold_test)\nexpected_tot = np.array([4])\ntot, expected_tot\n\n(array([0.]), array([4]))\n\n\nHere I wrapped everything into a function and added few initial lines of codes to transform a 1d array into a 2d array with the first dimension set to 1. This allows me to reuse the same code for the case in which we have a single waveform or multiple ones. The same applies for the threshold: if it is a scalar then it’s converted to a 1d array.\nLet’s test the function with a simple case: there are no points going below the threshold:\n\nwaveform_test = np.array([0, 0, 0, 0, 0])\nthreshold_test = -1\ntot = compute_time_over_threshold(waveform_test, threshold_test)\nexpected_tot = np.array([0])\ntot, expected_tot\n\n(array([0.]), array([0]))\n\n\nSo far so good. Now I want to test a simple case in which the time over threshold should be a simple scalar value.\n\nwaveform_test = np.array([0, -1, -1, -1, 0])\nthreshold_test = -0.5\ntot = compute_time_over_threshold(waveform_test, threshold_test)\nexpected_tot = np.array([3])\ntot, expected_tot\n\n(array([3.]), array([3]))\n\n\nThe time over threshold is working fine in this case. Few other corners cases: all samples are below threshold, so the computed time over threshold should be equal to the window length\n\nwaveform_test = np.array([-1, -1, -1, -1, -1])\nthreshold_test = -0.5\ntot = compute_time_over_threshold(waveform_test, threshold_test)\nexpected_tot = np.array([4])\ntot, expected_tot\n\n(array([0.]), array([4]))\n\n\nIn this case the time over threshold is not computed properly. The issue arises due to the way xlb is handled. In particular, in line 40, the part xlb = np.where(xlb != window_length - 1, xlb, 0) is setting the values of xlb to 0 in case xlb == window_length - 1. The problem is that there are two cases in which xlb could be equal to window_length - 1. One case is when the last sample is actually the last crossing the threshold. The second is the one where no samples are crossing the threshold, so np.argmax() returns the first item of the array, which in the reversed case is window_length - 1.\nFor this reason, it is important to distinguish the two cases and handle them separately. In the case of two conditions xlb == window_length - 1 and np.all(samples_below_threshold == True, axis=1) happening at the same time then all the samples are already below the threshold, thus the time over threshold will be equal to the window_length. Instead, in the case of xlb == window_length - 1 and np.any(samples_below_threshold == True, axis=1) it means there is at least one sample below the threshold so we can calculate the time over threshold as expected. It could be all the samples but in that case we would fall into the previous condition, so it should be already handled.\nAdditionally, xla should be also modified: if xlb == window_length - 1 then it means the last sample is still below the threshold, so xla can be set to be equal to xlb in order to compute the time over threshold on the last point (which is a segment delimited by \\(x_{la}, x_{lb}\\), so a segment with length 0). Otherwsise, xlb can be set to xla + 1 as usual.\nBelow the updated code:\n\ndef compute_time_over_threshold(data: np.ndarray, threshold: float | int | np.ndarray) -&gt; np.ndarray: \n    \"\"\"Compute the time over threshold for a 1d or 2d array given a fixed threshold. The\n    time over threshold is defined as the difference between the intersection of the\n    threshold with the segments of the samples: the last - the first segments\n    crossing this threshold define the tot. In case no samples cross the\n    tot, the resulting value is 0.as_integer_ratio\n    \n    Parameters\n    ----------\n    data: ndarray\n        1d or 2d array of shape (n_waveforms, n_samples)\n    threshold: float or int or ndarray (n_waveforms)\n        a fixed threshold for all the waveforms or a set of threshold\n        for each waveform\n\n    Returns\n    -------\n    out: ndarray\n        a 1d array of the time over thresholds of shape (n_waveforms)\n    \"\"\"\n    # In case of a 1d array of shape (n_samples), convert it to a 2d\n    # array of shape (1, n_samples), so that the code can be used for\n    # both 1d and 2d array\n    if data.ndim == 1:\n        waveforms = data[None, :]\n    else:\n        waveforms = data\n    # The same operation for threshold:\n    if not isinstance(threshold, np.ndarray):\n        threshold = np.array([threshold])\n    \n    samples_below_threshold = waveforms &lt; threshold[:, None]\n    window_length = waveforms.shape[1]\n\n    xfb = np.argmax(samples_below_threshold, axis=1) - 1\n    xfb = np.where(xfb != -1, xfb, 0)\n    xlb = window_length - np.argmax(samples_below_threshold[:, ::-1], axis=1) - 1\n    # Notice I removed the np.where() condition on xlb\n\n    yfb = np.take_along_axis(waveforms, xfb[:, None], axis=1).flatten()\n    ylb = np.take_along_axis(waveforms, xlb[:, None], axis=1).flatten()\n\n    xfa = xfb + 1\n    yfa = np.take_along_axis(waveforms, xfa[:, None], axis=1).flatten()\n    # Here I put a condition on xla: if xlb is the last item, then xla\n    # should be the same as xlb, so the segment (xlb-xla) is of length 0\n    xla = np.where(xlb == window_length - 1, xlb, xlb + 1)\n    yla = np.take_along_axis(waveforms, xla[:, None], axis=1).flatten()\n\n    xf = xfb + (np.divide((threshold - yfb) , (yfa - yfb), out=np.zeros_like(yfa, dtype='float64'), where=((yfa - yfb) != 0)))\n    xl = xlb + (np.divide((threshold - ylb) , (yla - ylb), out=np.zeros_like(yfa, dtype='float64'), where=((yla - ylb) != 0)))\n    # Use np.select() as a case statement:\n    time_over_threshold = np.select(condlist=[\n        (xlb != window_length - 1) & (np.all(samples_below_threshold == True, axis=1)),  # all points crossing threshold\n        (xlb != window_length - 1) & (np.any(samples_below_threshold == True, axis=1)),  # at least one point crossing threshold\n        (xlb == window_length - 1) & (np.all(samples_below_threshold == False, axis=1)), # no real points crossing threshold\n        (xlb == window_length - 1) & (np.any(samples_below_threshold == True, axis=1)), # last point crossing threshold\n    ], choicelist=[\n        window_length,\n        xl-xf,\n        0, \n        xl-xf\n    ])\n    \n    return time_over_threshold\n\nAs you can see I removed the np.where() condition onf xlb and added one on xla. If I didn’t, then when xlb is equal to window_length - 1, xla would be equal to window_length, resulting in an error when trying to access the array since the last item is at window_length - 1.\nAlso, I added np.select() which I usually think of as a n-dimensional version of a case statement. In case xlb != window_length - 1 there could be either all points crossing the threshold or at least one, so the time over threshold would be either the whole window_length or the computed xl - xf value.\nIn case xlb == window_length - 1 then there could be either no points at all crossing the threshold (time_over_threshold = 0) or there could be only the lat one crossing it, so the time over threshold would be again computed in the standard way of xl - xf.\nNow the case in which the first sample is already below the threshold but not the last is correctly handled:\n\nwaveform_test = np.array([-1, -1, -1, 0, 0])\nthreshold_test = -0.5\ntot = compute_time_over_threshold(waveform_test, threshold_test)\nexpected_tot = np.array([2.5])\ntot, expected_tot\n\n(array([2.5]), array([2.5]))\n\n\nHere below you can find the final the final code I use for the time over threshold calculation and few test cases:\n\n\nCode\ndef compute_time_over_threshold(data: np.ndarray, threshold: float | int | np.ndarray) -&gt; np.ndarray: \n    \"\"\"Compute the time over threshold for a 1d or 2d array given a fixed threshold. The\n    time over threshold is defined as the difference between the intersection of the\n    threshold with the segments of the samples: the last - the first segments\n    crossing this threshold define the tot. In case no samples cross the\n    tot, the resulting value is 0.as_integer_ratio\n    \n    Parameters\n    ----------\n    data: ndarray\n        1d or 2d array of shape (n_waveforms, n_samples)\n    threshold: float or int or ndarray (n_waveforms)\n        a fixed threshold for all the waveforms or a set of threshold\n        for each waveform\n\n    Returns\n    -------\n    out: ndarray\n        a 1d array of the time over thresholds of shape (n_waveforms)\n    \"\"\"\n    # In case of a 1d array of shape (n_samples), convert it to a 2d\n    # array of shape (1, n_samples), so that the code can be used for\n    # both 1d and 2d array\n    if data.ndim == 1:\n        waveforms = data[None, :]\n    else:\n        waveforms = data\n    # The same operation for threshold:\n    if not isinstance(threshold, np.ndarray):\n        threshold = np.array([threshold])\n    \n    samples_below_threshold = waveforms &lt; threshold[:, None]\n    window_length = waveforms.shape[1]\n\n    xfb = np.argmax(samples_below_threshold, axis=1) - 1\n    xfb = np.where(xfb != -1, xfb, 0)\n    xlb = window_length - np.argmax(samples_below_threshold[:, ::-1], axis=1) - 1\n\n    yfb = np.take_along_axis(waveforms, xfb[:, None], axis=1).flatten()\n    ylb = np.take_along_axis(waveforms, xlb[:, None], axis=1).flatten()\n\n    xfa = xfb + 1\n    yfa = np.take_along_axis(waveforms, xfa[:, None], axis=1).flatten()\n    xla = np.where(xlb == window_length - 1, xlb, xlb + 1)\n    yla = np.take_along_axis(waveforms, xla[:, None], axis=1).flatten()\n\n    xf = xfb + (np.divide((threshold - yfb) , (yfa - yfb), out=np.zeros_like(yfa, dtype='float64'), where=((yfa - yfb) != 0)))\n    xl = xlb + (np.divide((threshold - ylb) , (yla - ylb), out=np.zeros_like(yfa, dtype='float64'), where=((yla - ylb) != 0)))\n    time_over_threshold = np.select(condlist=[\n        (xlb != window_length - 1) & (np.all(samples_below_threshold == True, axis=1)),  # all points crossing threshold\n        (xlb != window_length - 1) & (np.any(samples_below_threshold == True, axis=1)),  # at least one point crossing threshold\n        (xlb == window_length - 1) & (np.all(samples_below_threshold == False, axis=1)), # no real point crossing threshold\n        (xlb == window_length - 1) & (np.any(samples_below_threshold == True, axis=1)), # last point\n    ], choicelist=[\n        window_length,\n        xl-xf,\n        0,\n        xl-xf\n    ])\n    \n    return time_over_threshold\n\n\nFew test cases here below:\n\n\nCode\nwaveform_test = np.array([0, 0, -1, -4, -4, -3, -1, 0, 0])\nthreshold_test = 1\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([8.])\n\n\n\n\nCode\nwaveform_test = np.array([0, 0, -1, -4, -4, -3, -1, 0, 0])\nthreshold_test = -10\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([0.])\n\n\n\n\nCode\nwaveform_test = np.array([-2, -1.5, -1, -4, -4, -3, -2, 0, 0])\nthreshold_test = -1\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([4.5])\n\n\n\n\nCode\nwaveform_test = np.array([0, 0, -1, -4, -4, -3, -1, 0, -4])\nthreshold_test = -2\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([5.66666667])\n\n\n\n\nCode\nwaveform_test = np.array([0, 0, -1, -4, -4, -3, -1, 0, 0])\nthreshold_test = -2\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([3.16666667])\n\n\n\n\nCode\nwaveform_test = np.array([0, 0, -1, -4, -4, -3, -2, 0, 0])\nthreshold_test = -2.2\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([3.4])\n\n\n\n\nCode\nwaveform_test = np.array([\n    [0, 0, -1, -4, -4, -3, -1.5, 0, 0],\n    [0, 0, -1, 2, 1, -2, 1.5, 0, 0],\n])\nthreshold_test = -2.2\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([3.13333333, 0.        ])\n\n\n\n\nCode\nwaveform_test = np.array([\n    [0, 0, -1, -4, -4, -3, -1.5, 0, 0],\n    [0, 0, -1, 2, 1, -2, 1.5, 0, 0],\n])\nthreshold_test = np.array([-2.2, -1])\ncompute_time_over_threshold(waveform_test, threshold_test)\n\n\narray([3.13333333, 0.61904762])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "G.R.'s Blog",
    "section": "",
    "text": "Display data from a SQL source on the web with Tabulator and FastAPI\n\n\nWith server side pagination and filtering\n\n\n\n\npython\n\n\nweb\n\n\nfastapi\n\n\ntabulator\n\n\nSQL\n\n\n \n\n\n\n\nSep 23, 2023\n\n\n31 min\n\n\n\n\n\n\n  \n\n\n\n\nMy take on overlapping densities (a.k.a. ridge) plots\n\n\nA consistent style to produce distribution plots ready for publications.\n\n\n\n\npython\n\n\nmatplotlib\n\n\nplot\n\n\nvisualization\n\n\nridge-plot\n\n\n \n\n\n\n\nOct 17, 2022\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nTime Over Threshold in signal processing\n\n\nA useful indicator of your signal dynamics\n\n\n\n\npython\n\n\nmatplotlib\n\n\nscipy\n\n\nnumpy\n\n\nsignal processing\n\n\nsignal analysis\n\n\n \n\n\n\n\nSep 30, 2022\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nMy matplotlib stylesheet\n\n\nSometimes you need to make your matplotlib plots look like they were generated by other libraries\n\n\n\n\npython\n\n\nmatplotlib\n\n\nplot\n\n\nvisualization\n\n\nstyle\n\n\n \n\n\n\n\nSep 18, 2022\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nMultiple Y axes with matplotlib\n\n\nBecause sometimes right y axes are not enough\n\n\n\n\npython\n\n\nmatplotlib\n\n\nplot\n\n\nvisualization\n\n\n \n\n\n\n\nSep 8, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nA post made from a jupyter notebook\n\n\nOr how I fell in love with Quarto\n\n\n\n\njupyter\n\n\nnotebook\n\n\npython\n\n\nquarto\n\n\n \n\n\n\n\nSep 7, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/plots-with-multiple-y-axes/index.html",
    "href": "posts/plots-with-multiple-y-axes/index.html",
    "title": "Multiple Y axes with matplotlib",
    "section": "",
    "text": "Introduction\nWhen working with gas systems I make intense use of the WinCC-OA trending tool, which allows to plot up to 8 time series on the same panel. Each time series tpyically corresponds to the value read by a sensor of a plant so it may have different units and range. The trending tool allows to have multiple Y axes on the left side which can be adjusted in terms of range and offset.\nI find this feature particularly helpful, especially when there is the need to quickly and explore and correlate readings of sensor from different parts of the plant\n\n\n\nWinCC-OA/UNICOS trending tool\n\n\nTools like matplotlib and plotly make it easy to work with multiple series plotted on the same data but I found a bit cumbersome trying to visualize data having different scales on the same plot.\n\n\nThe problem of visualizing many series\nAssuming we have a very simple set of \\((x, y_n)\\) series a simple plot with matplotlib may look like this:\n\nimport matplotlib.pyplot as plt\nimport random as rnd\n\nx = range(100)\ny1 = [rnd.random() for i in x]\ny2 = [rnd.random() * 10 for i in x]\ny3 = [rnd.random() * 20 + 10 for i in x]\ny4 = [rnd.random() * 100 + 50 for i in x]\n\nfig, ax = plt.subplots()\nfor y in [y1, y2, y3, y4]:\n    ax.plot(x, y, '.-')\n\n\n\n\nFigure 1: A matplotlib plot with four series plotted together.\n\n\n\n\nNote that in Figure Figure 1 each time series has a different standard deviation, thus different ranges may be needed. This is often easily accomplished by plotting each series in a different subplots. However, subplots make it more difficult to visually compare and align series, especially when time-based. For example, see subplots here:\n\nx = range(100)\ny1 = [rnd.random() if i &lt; 30 else rnd.random() + 1 for i in x]\ny2 = [rnd.random() * 10 if i &lt; 33 else rnd.random() * 10 + 10 for i in x]\ny3 = [rnd.random() * 20 + 10 if i &lt; 30 else rnd.random() * 20 + 30 for i in x]\ny4 = [rnd.random() * 100 + 50 if i &lt; 27 else rnd.random() * 100 + 180 for i in x]\n\nfig, ax = plt.subplots()\nfor y in [y1, y2, y3, y4]:\n    ax.plot(x, y, '.-')\n\n\n\n\nMultiple series with an offset at arbitrary x.\n\n\n\n\nHere I have added an offset to each series. Two series, y1 and y3 have a change point at the same index, while the other two have a change point at slightly different xs. We could plot each series in a subplots, perhaps vertically stacked:\n\nseries = [y1, y2, y3, y4]\nfig, axs = plt.subplots(len(series), 1, figsize=(6, 4*len(series)))\nfor ix, y in enumerate(series):\n    ax = axs.flat[ix]\n    ax.plot(x, y, '.-')\n    ax.set_title(f'y{ix+1}')\n\n\n\n\nFigure 2: Series plotted on different subplots.\n\n\n\n\nIn figure Figure 2 you can see that each series as an offset when adding a proper range on the y axis. However, it is still a bit difficult to understand the real indexes of the offset. I would like to understand which come first and which comes later.\n\n\nAdding multiple Y axes to matplotlib plots\nWe can starting adding multiple axes by taking inspiration from the Matplotib documentation using spines, Parasite Axes and another Parasite axis demo.\nThe idea is to use ax.twinx() to create an additional axes. As the documentation says, &gt; Create a new Axes with an invisible x-axis and an independent y-axis positioned opposite to the original one (i.e. at right).\nAlthough twinx() is used to create a secondary axis on the right position I could use it to create a secondary axis and leave the spines of the axis only on the left. I can use set_position() on the spines object to shift the spines on left:\n\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax3 = ax1.twinx()\nax4 = ax1.twinx()\naxs = [ax1, ax2, ax3, ax4]\n\nfor ix, (ax, y) in enumerate(zip(axs, series)):\n    ax.plot(x, y, label=f'y{ix}', color=f'C{ix}')\nax.legend();\n\n\n\n\nFigure 3: Single plot with multiple series and secondary axes.\n\n\n\n\nAs you can see in Figure Figure 3 we can understand the index at which each change point of the series is happening. The only issue is that the y axes on the right are overlapping between each other.\nMy goal is to have these secondary y-axes on the left for easier reading. Actually, if you inspect the source of how twinx() is defined, it calls Axes._make_twin_axes() and sets later the tick position on the right using YAxis.tick_right() and YAxis.set_label_position('right'). It would be nice if twinx() would not assume that we want the axes to the right and instead allowed to pass a parameter which decised the position.\nHere below I leave a minimum working example I could think of:\n\nfig, axes = plt.subplots()\nfor ix, y in enumerate(series):\n    # If we have to plot the first series we use \n    # The axes created by plt.subplots() earlier\n    if ix == 0: \n        ax = axes\n    else:\n        # It's not the first series: we need to\n        # create a twin axes\n        ax = axes.twinx()\n    # Set the ticks of the axis to the left\n    ax.yaxis.tick_left()\n    # Set the labels of the axes to the lef\n    ax.yaxis.set_label_position('left')\n    ax.yaxis.set_offset_position('left')\n    # Offset the position of he ticks and labels\n    # by some % of the axes avoid overlapping of axes\n    ax.spines['left'].set_position(('outward', 40 * ix))\n    # Plot the actual data\n    ax.plot(x, y, color=f'C{ix}')\n    ax.spines['left'].set_color(f'C{ix}')\n    ax.tick_params(axis='y', colors=f'C{ix}')\n\n\n\n\nEt voilà, here I have a plot similar to the WinCC-OA one. I could improve the plot a bit by using the same number of ticks for each axes. I would do this using the LinearLocator class:\n\n\nCode\nimport matplotlib.ticker as mt\n\nfig, axes = plt.subplots()\nfor ix, y in enumerate(series):\n    # If we have to plot the first series we use \n    # The axes created by plt.subplots() earlier\n    if ix == 0: \n        ax = axes\n    else:\n        # It's not the first series: we need to\n        # create a twin axes\n        ax = axes.twinx()\n    # Set the ticks of the axis to the left\n    ax.yaxis.tick_left()\n    # Set the labels of the axes to the left\n    ax.yaxis.set_label_position('left')\n    ax.yaxis.set_offset_position('left')\n    # Offset the position of he ticks and labels\n    # by some % of the axes avoid overlapping of axes\n    ax.spines['left'].set_position(('outward', 40 * ix))\n    # Plot the actual data\n    ax.plot(x, y, color=f'C{ix}')\n    # Set the colors of the ticks, labels and spines to be\n    # the same of the associated series\n    ax.spines['left'].set_color(f'C{ix}')\n    ax.tick_params(axis='y', colors=f'C{ix}')\n    # Use a tick locator to have the same number of ticks\n    ax.yaxis.set_major_locator(mt.LinearLocator(11))\n    # And format the labels to have only one digit after the decimals\n    ax.yaxis.set_major_formatter(mt.StrMethodFormatter('{x:.1f}'))\n\n\n\n\n\n\n\nConclusions\nI find very useful for myself to provide a minimal example of having a plot with multiple axes, though a final plot may require more subtle adjustements. Keypoints to have multiple y axes:\n\nUse twinx() to create an additional axis\nSet ticks, labels and offest positions to the right: ax.yaxis.tick_left(), ax.yaxis.set_label_position('left'), ax.yaxis.set_offset_position('left')\nAdjust the offset of the spines to the left using points, percentage or data coordinate. In the case of points: ax.spines['left'].set_position()\nChange spines, tick and label colors to the same of the series for better readability: ax.spines['left'].set_color(color), ax.tick_params(axis='y', colors=color)\nOptionally adjust the number of ticks to be the same for all the axes: use a LinearLocator class with a fixed number of points"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html",
    "href": "posts/my-matplotlib-stylesheet/index.html",
    "title": "My matplotlib stylesheet",
    "section": "",
    "text": "When working on R&D of gasesous detectors I tend to produce lots of plots for internal group discussions, conference presentations and publication publishing. Since most of these plots shows performance relative to gaseous detectors and since I am based at CERN I get to see lot of similar plots made by some other groups using ROOT, the de-facto standard analysis tool for high energy physics.\nIf I really have to be honest I have never liked ROOT so much, although I understand it provides several useful tools, from plotting data and fitting functions to GUI toolkits. Most, if not all the plots regarding the discovery of the Higgs boson were produced using ROOT. The layout of ROOT plots is then recognized and approved by the majority of the community in high energy physics.\nIf we have to compare a default plot made using root versus a default plot made with matplotlib, this would be the result\n\n\nCode\nimport matplotlib.pyplot as plt\nimport ROOT as rt\nimport numpy as np\n\nx = np.array([321.0, 860.0, 777.0, 562.0, 374.0, 132.0, 816.0, 220.0, 253.0])\ny = np.array([134.0,307.0,299.0,213.0,135.0,50.0,297.0,85.0,95.0])\n\n# Let's just make sure we are using the default settings now\nplt.style.use('default')\n\n# Matplotlib\nfig, ax = plt.subplots()\nax.set(title='Rate vs. Currents', xlabel='Rate [Hz/cm$^2$]', ylabel='Currents [uA]')\nax.plot(x, y, '.')\nax.axhline(150, color='r')\n\n# ROOT\ncanvas = rt.TCanvas(\"rate_vs_current\", \"Rate vs. Currents\")\ngraph = rt.TGraph(len(x), x, y)\ngraph.SetTitle(\"Rate vs. Currents\")\ngraph.SetMarkerStyle(7)\ngraph.GetXaxis().SetTitle(\"Rate [Hz/cm^{2}]\")\ngraph.GetYaxis().SetTitle(\"Currents [uA]\")\nline = rt.TLine(graph.GetXaxis().GetXmin(), 150, graph.GetXaxis().GetXmax(), 150)\nline.SetLineColor(rt.kRed)\ngraph.Draw('AP')\nline.Draw(\"L\")\ncanvas.Draw()\n\n\n\n\n\n\n\nFigure 1: Default matplotlib plot\n\n\n\n\n\n\n\nFigure 2: Default ROOT plot"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#lines",
    "href": "posts/my-matplotlib-stylesheet/index.html#lines",
    "title": "My matplotlib stylesheet",
    "section": "LINES",
    "text": "LINES\nIn the LINES section the only default value that I have changed is the lines.linewidth set to 0.5, instead of the 1.5 default value."
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#patches",
    "href": "posts/my-matplotlib-stylesheet/index.html#patches",
    "title": "My matplotlib stylesheet",
    "section": "PATCHES",
    "text": "PATCHES\nIn the PATCHES section I have also decrease the patch.linewidth to 0.5"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#font",
    "href": "posts/my-matplotlib-stylesheet/index.html#font",
    "title": "My matplotlib stylesheet",
    "section": "FONT",
    "text": "FONT\nFor the font I haven’t found any easy way to add a web font into matplotlib without adding some lines of code. For this reason I have installed GNU Free font and modified the order of the sans-serif font to be: Helvetica, FreeSans, Nimbus Sans, DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Avant Garde, sans-serif\nIn the FONT section I have set font.weight to regular by default."
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#axes-and-axis",
    "href": "posts/my-matplotlib-stylesheet/index.html#axes-and-axis",
    "title": "My matplotlib stylesheet",
    "section": "AXES and AXIS",
    "text": "AXES and AXIS\nIn the AXES and AXIS sections I tweaked multiple lines. I will report only the modified lines:\n\n\nmy_matplotlibrc.txt\n\n# AXES\naxes.grid           : False   ## display grid or not\naxes.labelsize      : x-large  ## fontsize of the x any y labels\naxes.labelweight    : regular  ## weight of the x and y labels\naxes.formatter.useoffset      : False    ## If True, the tick label formatter\naxes.formatter.offset_threshold : 2     ## When useoffset is True, the offset\n...\n# AXIS\nxaxis.labellocation: right  # alignment of the xaxis label: {left, right, center}\nyaxis.labellocation: top  # alignment of the yaxis label: {bottom, top, center}"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#ticks",
    "href": "posts/my-matplotlib-stylesheet/index.html#ticks",
    "title": "My matplotlib stylesheet",
    "section": "TICKS",
    "text": "TICKS\nMatplotlib’s tick layout is what differs the most from ROOT default’s style. In this case the modified lines were the following:\n\n\nmy_matplotlibrc.txt\n\nxtick.top            : True  ## draw ticks on the top side\nxtick.major.size     : 8    ## major tick size in points\nxtick.minor.size     : 4      ## minor tick size in points\nxtick.major.width    : 0.5    ## major tick width in points\nxtick.minor.width    : 0.5    ## minor tick width in points\nxtick.major.pad      : 6    ## distance to major tick label in points\nxtick.direction      : in    ## direction: in, out, or inout\nxtick.minor.visible  : True  ## visibility of minor ticks on x-axis\nytick.right          : True   ## draw ticks on the right side\nytick.major.size     : 8      ## major tick size in points\nytick.minor.size     : 4      ## minor tick size in points\nytick.major.width    : 0.5    ## major tick width in points\nytick.minor.width    : 0.5    ## minor tick width in points\nytick.major.pad      : 6    ## distance to major tick label in points\nytick.direction      : in     ## direction: in, out, or inout\nytick.minor.visible  : True  ## visibility of minor ticks on y-axis"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#grid",
    "href": "posts/my-matplotlib-stylesheet/index.html#grid",
    "title": "My matplotlib stylesheet",
    "section": "GRID",
    "text": "GRID\nAlthough I used to love grid, they might make the plot a bit “heavy” to read when there are several points and lines. For this reason I kept grids disabled:\n\n\nmy_matplotlibrc.txt\n\ngrid.color       :   k    ## black\ngrid.linestyle   :   --         ## dashed\ngrid.linewidth   :   0.5       ## in points"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#legend",
    "href": "posts/my-matplotlib-stylesheet/index.html#legend",
    "title": "My matplotlib stylesheet",
    "section": "LEGEND",
    "text": "LEGEND\nRegardin the legend handling I got some inspirations by having a look at some common plots used within the community I am working with:\n\n\nmy_matplotlibrc.txt\n\nlegend.frameon       : True     ## if True, draw the legend on a background patch\nlegend.framealpha    : None      ## legend patch transparency\nlegend.edgecolor     : inherit      ## background patch boundary color\nlegend.fancybox      : False     ## if True, use a rounded box for the\n                                 ## legend background, else a rectangle\nlegend.scatterpoints : 3        ## number of scatter points\nlegend.fontsize      : large\nlegend.handlelength  : 0.7      ## the length of the legend lines\nlegend.handleheight  : 1      ## the height of the legend handle\nlegend.handletextpad : 1.2      ## the space between the legend line and legend text\nlegend.borderaxespad : 1      ## the border between the axes and legend edge"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#figure",
    "href": "posts/my-matplotlib-stylesheet/index.html#figure",
    "title": "My matplotlib stylesheet",
    "section": "FIGURE",
    "text": "FIGURE\nAlthough there are really no standard on the figure sizes I like a default 3:2 ratio and a minimum DPI of 300:\n\n\nmy_matplotlibrc.txt\n\nfigure.figsize   : 6, 4       ## figure size in inches\nfigure.dpi       : 300        ## figure dots per inch"
  },
  {
    "objectID": "posts/my-matplotlib-stylesheet/index.html#other-tweakings",
    "href": "posts/my-matplotlib-stylesheet/index.html#other-tweakings",
    "title": "My matplotlib stylesheet",
    "section": "Other tweakings",
    "text": "Other tweakings\nI have set the cap size of the error bars, as by default is 0 and set the histogram binning to \"auto\":\n\n\nmy_matplotlibrc.txt\n\nerrorbar.capsize : 1.5            ## length of end cap on error bars in pixels\nhist.bins : auto                 ## The default number of histogram bins.\n                                  ## If Numpy 1.11 or later is\n                                  ## installed, may also be `auto`"
  },
  {
    "objectID": "posts/overlapping-densities-plots/index.html",
    "href": "posts/overlapping-densities-plots/index.html",
    "title": "My take on overlapping densities (a.k.a. ridge) plots",
    "section": "",
    "text": "Introduction\nRecently I had to produce some plots showing a distribution of the charge that is induced by an ionizing particle passing through a gasesous detector. Such distribution is highly dependent on the electric field in the gas medium and the gas mixture itself. Since I am studying eco-friendly gas mixture for a particular kind of the detector I wanted to compare the charge distribution that the detectors have when operated with different gas mixture. In this context, I initially though about overlapping the charge distributions, but there may be some features about the distribution that can easily get lost when plotting one histogram on top of the other.\nIn addition, I wanted to produce some histograms with a style consistent to the other plots that I usually make, so I am using my custom matplotlib stylesheet, as explained in my previous blog post\nIn order to show the issue that I had to fight with, I have prepared a .csv file containing two columns: one colum represents the charge that will be plotted in the form of a histogram. The second column is categorical label indicating to which gas mixture the measured charge belongs to.\nAlso, I often use seaborn to help me produce these kind of plots.\nLet me start by showing the different charge distributions:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nplt.style.use('https://gitlab.cern.ch/-/snippets/2223/raw/master/rpcecogas.mplstyle')\ndf = pd.read_csv('charge_distribution.csv', index_col=0)\n\ndf.head()\n\n\n\n\n\n\n\n\n\ncharge\nlabel\n\n\n\n\n0\n2.521321\nStandard\n\n\n1\n2.074056\nStandard\n\n\n2\n1.207791\nStandard\n\n\n3\n0.663411\nStandard\n\n\n4\n0.618327\nStandard\n\n\n\n\n\n\n\nIf I try to plot a histogram of charge with a color based on the label category I get this:\n\n\nCode\nax = sns.histplot(data=df, x='charge', hue='label', log_scale=True, \n             stat='density', common_norm=False, element='step',\n             bins=np.linspace(-1, 4, 200), fill=False)\nax.get_legend().set_title('Label')\n\n\n\n\n\nCharge distribution of different gas mixtures\n\n\n\n\nThe above plot was made using the seaborn histplot() function, using a logarithmic scale on the x axis. In this particular case one can see that the blue distribution has a nice, single, bell shape, while the other three distributions, which represent gas mixtures with the addition of CO\\(_2\\) to the standard gas mixture, have a more complex behaviour. Although it looks clear that with an increase of CO\\(_2\\) the region with a charge higher than 20 a.u. is increasing, the behaviour on lower region is difficult to understand due to the overlapping lines, especially in the region around 2 a.u.\nThe idea was then to try to make a ridge plot, where the the histograms are shifted of a fixed offest along the y-axis. In this case, my starting point was a plot made with seaborn, although there are plain matplotlib solutions. The seaborn documentation on ridge plots suggests to use a FacetGrid object and make use of the method FacetGrid.map to use a seaborn drawing functions on top of the figure created by the FacetGrid. The FacetGrid object pass a row parameter which should contains the name of a column of the dataframe being used. The row parameter allows to create unique rows of plots depending on how many unique values the dataframe column has.\nIn addition, I also want the lines to be colored differently depending on the label, so I will also make use of the hue parameter.\nBy only using map I get this results:\n\n\nCode\ng = sns.FacetGrid(data=df, hue='label', row='label', aspect=2, height=2)\n# Use the seaborn histplot() function to create histograms\n# for each row. In this case I am passing the same\n# arguments of the plot made before\ng.map(sns.histplot, 'charge', log_scale=True, \n             stat='density', common_norm=False, element='step',\n             bins=np.linspace(-1, 4, 200), fill=False)\n\n\n\n\n\nThis plot can be considered as an improvement with respect to ?@fig-default-histogram as it allows to clearly see the shape difference between the charge distributions. It is indeed possible to observe a a less sharp charge distribution in the last histogram (the red one) if compared to the previous one (the green house).\nThe only downside of such a plot is its vertical space. There are axes frames, axes titles and ticks that eat up a significant amount of space. So in this case I would get rid of the top, bottom and left axes frames and titles, except for the last one:\n\n\nCode\ng = sns.FacetGrid(data=df, hue='label', row='label', aspect=2, height=2)\n# Use the seaborn histplot() function to create histograms\n# for each row. In this case I am passing the same\n# arguments of the plot made before\ng.map(sns.histplot, 'charge', log_scale=True, \n             stat='density', common_norm=False, element='step',\n             bins=np.linspace(-1, 4, 200), fill=False)\n# Remove all left, top and right frames\ng.despine(left=True, top=True, bottom=True)\n\n\n\n\n\nNow I would like to remove the ticks on the sides, bottom and top as I did for the frames. To do this, I would set the ticks length to 0 and the tick labels to an empty list:\n\n\nCode\n# Adding a matplotlib rc_context to disable ticks\ng = sns.FacetGrid(data=df, hue='label', row='label', aspect=2, height=2)\n# Use the seaborn histplot() function to create histograms\n# for each row. In this case I am passing the same\n# arguments of the plot made before\ng.map(sns.histplot, 'charge', log_scale=True, \n            stat='density', common_norm=False, element='step',\n            bins=np.linspace(-1, 4, 200), fill=False)\n# Remove all left, top and right frames\ng.despine(left=True, top=True, bottom=True)\n# Manually removing ticks\nfor ax in g.axes.ravel()[:-1]:\n    # Remove ticks by settings their length to 0\n    ax.tick_params(axis='both', length=0, which='both')\n\n# Use seaborn to remove y labels and titles\ng.set_axis_labels(\"Charge [pC]\", \"\")\ng.set_titles(\"\")\n# Remove y ticks from all axes\ng.set(yticklabels=[])\n# Remove the spines also on the last plot, except for the bottom one\nlast_ax = g.axes.flat[-1]\nsns.despine(left=True, top=True, bottom=False, ax=g.axes.flat[-1])\n# Use mpl apis to remove ticks\nlast_ax.tick_params(top=False, left=False, right=False, which='both')\n\n\n\n\n\nNow, what is left is to get the distributions closer to each other and eventually add a label to each of them, indicating to which category they belong:\n\n\nCode\ndef add_label(x, color, label):\n    \"\"\"Add a label to each mapped category\"\"\"\n    ax = plt.gca()\n    # The position of the label depends on the figure size and layout.\n    # For this reason, the x and y coordinates may need to be set manually.\n    ax.text(1, .2, label, color=color, fontsize=13, fontweight='bold',\n            ha=\"right\", va=\"center\", transform=ax.transAxes)\n\n# Minor adjustment on aspect and height: use a large aspect for broad\n# distributions and set height to 1 (this value depends also on the\n# matplotlib's dpi resolution)\ng = sns.FacetGrid(data=df, hue='label', row='label', aspect=4, height=1)\n\ng.map(sns.histplot, 'charge', log_scale=True, \n            stat='density', common_norm=False, element='step',\n            bins=np.linspace(-1, 4, 200), fill=False)\n\ng.despine(left=True, top=True, bottom=True)\n\nfor ax in g.axes.ravel()[:-1]:\n    ax.tick_params(axis='both', length=0, which='both')\n    \n# Map the label function to each 'hue' catgory of seaborn\ng.map(add_label, \"label\")\n# Make the subplots get close together by decreasing the horizontal space\ng.fig.subplots_adjust(hspace=-0.4)\n\ng.set_titles(\"\")\ng.set(yticklabels=[])\n# For some seaborn internal reason, the set_titles() function\n# should be called after the map function, otherwise the titles\n# won't be removed\ng.set_axis_labels(\"Charge [pC]\", \"\")\nlast_ax = g.axes.flat[-1]\nsns.despine(left=True, top=True, bottom=False, ax=last_ax)\nlast_ax.tick_params(top=False, left=False, right=False, which='both')\n\n# Set a transparent facecolor, otherwise the white backround of a \n# subplot will cover the ones behind\ng.set(facecolor=[0,0,0,0])\n# Make the sublots closer by removing some horizontal space between\n# them\ng.fig.subplots_adjust(hspace=-0.3)\n\n\n\n\n\nEt voilà, the plot is ready. What should be mentioned is that in this case I have combined both seaborn and matplotlib’s API’s, although I’m pretty sure there is a cleaner way with matplotlib APIs only. For the code, there are few caveats that should be taken into account:\n\nThe addition of a label is done by setting the coordinates relative to the subplots with hand-crafted values. I haven’t found yet a better solution;\nThe addition of a label using seaborn’s FacetGrid.map() function should be done before setting titls and axes labels using seaborn FacetGrid.set(). I haven’t digged down to see why to be honest, but if one first sets the titles to \"\" and later uses the map() function the titles are coming back.\nThe subplots should have a transparent face color. My default stylesheet has a white facecolor. If I don’t change the facecolor, the subplots on the front will partially cover the ones in the back.\n\nAlso, one can notice that this kind of plot is providing some easier way to compare different distributions when they have different skeweness and shapes.\n\n\nConclusions\nIn this post I showed how I am using seaborn and matplotlib to create my own, publication-ready, ridge plot, which is at the end a simple overlapping of histograms with some additional styles applied. To sum up, the steps that I have applied to get this plot:\n\nDespine everything if using seaborn\nSet the xtick length to 0 for all the axes except the last one\nSet all axes titles to “” and all yticklabels to an empty list\nRe apply despine to leave the bottom spines for the last axis\nPut back the bottom spine on the last axis using again sns.despine()\nRemove ticks from top, left, right faces of the last axis\nSet the facecolor of the axes to be transparent\nGets the subplots closer using plt.subplots_adjust() and a negative hspace\n\nThere may be a more straightforward way to do this and in that I case I would be glad to see it, but so far it is the only way I found to get a plot that does what I needed. I found this kind of visualization very interesting and a bit more easy to use in publications with respect to the ridge plots showed in other tutorials and documentations as it doesn’t have any fill color on the distribution, it’s consistent with the applied stylesheet and it is relatively uncluttured."
  }
]